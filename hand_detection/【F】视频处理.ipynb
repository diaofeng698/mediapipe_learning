{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excess-equipment",
   "metadata": {},
   "source": [
    "# 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "productive-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opencv-python\n",
    "import cv2\n",
    "# mediapipe人工智能工具包\n",
    "import mediapipe as mp\n",
    "# 进度条库\n",
    "from tqdm import tqdm\n",
    "# 时间库\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-preservation",
   "metadata": {},
   "source": [
    "# 导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chief-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入solution\n",
    "mp_hands = mp.solutions.hands\n",
    "# 导入模型\n",
    "hands = mp_hands.Hands(static_image_mode=False,        # 是静态图片还是连续视频帧\n",
    "                       max_num_hands=3,                # 最多检测几只手\n",
    "                       min_detection_confidence=0.6,   # 置信度阈值\n",
    "                       min_tracking_confidence=0.5)    # 追踪阈值\n",
    "# 导入绘图函数\n",
    "mpDraw = mp.solutions.drawing_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cutting-hollywood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mmp_hands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstatic_image_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_num_hands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_complexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_detection_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_tracking_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "MediaPipe Hands.\n",
      "\n",
      "MediaPipe Hands processes an RGB image and returns the hand landmarks and\n",
      "handedness (left v.s. right hand) of each detected hand.\n",
      "\n",
      "Note that it determines handedness assuming the input image is mirrored,\n",
      "i.e., taken with a front-facing/selfie camera (\n",
      "https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped\n",
      "horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)\n",
      "to flip the image first for a correct handedness output.\n",
      "\n",
      "Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for\n",
      "usage examples.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Initializes a MediaPipe Hand object.\n",
      "\n",
      "Args:\n",
      "  static_image_mode: Whether to treat the input images as a batch of static\n",
      "    and possibly unrelated images, or a video stream. See details in\n",
      "    https://solutions.mediapipe.dev/hands#static_image_mode.\n",
      "  max_num_hands: Maximum number of hands to detect. See details in\n",
      "    https://solutions.mediapipe.dev/hands#max_num_hands.\n",
      "  model_complexity: Complexity of the hand landmark model: 0 or 1.\n",
      "    Landmark accuracy as well as inference latency generally go up with the\n",
      "    model complexity. See details in\n",
      "    https://solutions.mediapipe.dev/hands#model_complexity.\n",
      "  min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand\n",
      "    detection to be considered successful. See details in\n",
      "    https://solutions.mediapipe.dev/hands#min_detection_confidence.\n",
      "  min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
      "    hand landmarks to be considered tracked successfully. See details in\n",
      "    https://solutions.mediapipe.dev/hands#min_tracking_confidence.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/dg_demo/lib/python3.8/site-packages/mediapipe/python/solutions/hands.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "mp_hands.Hands?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-neighbor",
   "metadata": {},
   "source": [
    "# 处理单帧的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "macro-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(img):\n",
    "    \n",
    "    # 记录该帧开始处理的时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 获取图像宽高\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "\n",
    "    # 水平镜像翻转图像，使图中左右手与真实左右手对应\n",
    "    # 参数 1：水平翻转，0：竖直翻转，-1：水平和竖直都翻转\n",
    "    img = cv2.flip(img, 1)\n",
    "    # BGR转RGB\n",
    "    img_RGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # 将RGB图像输入模型，获取预测结果\n",
    "    results = hands.process(img_RGB)\n",
    "\n",
    "    if results.multi_hand_landmarks: # 如果有检测到手\n",
    "\n",
    "        handness_str = ''\n",
    "        index_finger_tip_str = ''\n",
    "        for hand_idx in range(len(results.multi_hand_landmarks)):\n",
    "\n",
    "            # 获取该手的21个关键点坐标\n",
    "            hand_21 = results.multi_hand_landmarks[hand_idx]\n",
    "\n",
    "            # 可视化关键点及骨架连线\n",
    "            mpDraw.draw_landmarks(img, hand_21, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 记录左右手信息\n",
    "            temp_handness = results.multi_handedness[hand_idx].classification[0].label\n",
    "            handness_str += '{}:{} '.format(hand_idx, temp_handness)\n",
    "\n",
    "            # 获取手腕根部深度坐标\n",
    "            cz0 = hand_21.landmark[0].z\n",
    "\n",
    "            for i in range(21): # 遍历该手的21个关键点\n",
    "\n",
    "                # 获取3D坐标\n",
    "                cx = int(hand_21.landmark[i].x * w)\n",
    "                cy = int(hand_21.landmark[i].y * h)\n",
    "                cz = hand_21.landmark[i].z\n",
    "                depth_z = cz0 - cz\n",
    "\n",
    "                # 用圆的半径反映深度大小\n",
    "                radius = max(int(6 * (1 + depth_z*5))*2, 0)\n",
    "\n",
    "                if i == 0: # 手腕\n",
    "                    img = cv2.circle(img,(cx,cy), radius, (0,0,255), -1)\n",
    "                if i == 8: # 食指指尖\n",
    "                    img = cv2.circle(img,(cx,cy), radius, (193,182,255), -1)\n",
    "                    # 将相对于手腕的深度距离显示在画面中\n",
    "                    index_finger_tip_str += '{}:{:.2f} '.format(hand_idx, depth_z)\n",
    "                if i in [1,5,9,13,17]: # 指根\n",
    "                    img = cv2.circle(img,(cx,cy), radius, (16,144,247), -1)\n",
    "                if i in [2,6,10,14,18]: # 第一指节\n",
    "                    img = cv2.circle(img,(cx,cy), radius, (1,240,255), -1)\n",
    "                if i in [3,7,11,15,19]: # 第二指节\n",
    "                    img = cv2.circle(img,(cx,cy), radius, (140,47,240), -1)\n",
    "                if i in [4,12,16,20]: # 指尖（除食指指尖）\n",
    "                    img = cv2.circle(img,(cx,cy), radius, (223,155,60), -1)\n",
    "        \n",
    "        scaler = 2 # 字体大小\n",
    "        img = cv2.putText(img, handness_str, (25 * scaler, 100 * scaler), cv2.FONT_HERSHEY_SIMPLEX, 1.25 * scaler, (255, 0, 255), 2 * scaler)\n",
    "        img = cv2.putText(img, index_finger_tip_str, (25 * scaler, 150 * scaler), cv2.FONT_HERSHEY_SIMPLEX, 1.25 * scaler, (255, 0, 255), 2 * scaler)\n",
    "        \n",
    "        # 记录该帧处理完毕的时间\n",
    "        end_time = time.time()\n",
    "        # 计算每秒处理图像帧数FPS\n",
    "        FPS = 1/(end_time - start_time)\n",
    "\n",
    "        # 在图像上写FPS数值，参数依次为：图片，添加的文字，左上角坐标，字体，字体大小，颜色，字体粗细\n",
    "        img = cv2.putText(img, 'FPS  '+str(int(FPS)), (25 * scaler, 50 * scaler), cv2.FONT_HERSHEY_SIMPLEX, 1.25 * scaler, (255, 0, 255), 2 * scaler)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-relaxation",
   "metadata": {},
   "source": [
    "# 视频逐帧处理（模板）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intellectual-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 视频逐帧处理代码模板\n",
    "# 不需修改任何代码，只需定义process_frame函数即可\n",
    "\n",
    "def generate_video(input_path='./videos/three-hands.mp4'):\n",
    "    filehead = input_path.split('/')[-1]\n",
    "    output_path = \"out-\" + filehead\n",
    "    \n",
    "    print('视频开始处理',input_path)\n",
    "    \n",
    "    # 获取视频总帧数\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_count = 0\n",
    "    while(cap.isOpened()):\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "        if not success:\n",
    "            break\n",
    "    cap.release()\n",
    "    print('视频总帧数为',frame_count)\n",
    "    \n",
    "    # cv2.namedWindow('Crack Detection and Measurement Video Processing')\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (int(frame_size[0]), int(frame_size[1])))\n",
    "    \n",
    "    # 进度条绑定视频总帧数\n",
    "    with tqdm(total=frame_count-1) as pbar:\n",
    "        try:\n",
    "            while(cap.isOpened()):\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    break\n",
    "\n",
    "                # 处理帧\n",
    "                # frame_path = './temp_frame.png'\n",
    "                # cv2.imwrite(frame_path, frame)\n",
    "                try:\n",
    "                    frame = process_frame(frame)\n",
    "                except:\n",
    "                    print('error')\n",
    "                    pass\n",
    "                \n",
    "                if success == True:\n",
    "                    # cv2.imshow('Video Processing', frame)\n",
    "                    out.write(frame)\n",
    "\n",
    "                    # 进度条更新一帧\n",
    "                    pbar.update(1)\n",
    "\n",
    "                # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    # break\n",
    "        except:\n",
    "            print('中途中断')\n",
    "            pass\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    print('视频已保存', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "oriental-airport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频开始处理 ./videos/three_hands.mp4\n",
      "视频总帧数为 303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 302/302 [00:13<00:00, 21.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频已保存 out-three_hands.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_video(input_path='./videos/three_hands.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artistic-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频开始处理 ./videos/start.mp4\n",
      "视频总帧数为 524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [00:23<00:00, 21.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频已保存 out-start.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_video(input_path='./videos/start.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-centre",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dg_demo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f29ded39bd84ccde43438c3db4c0b346c1217eab52822133991d7d581c3429a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
